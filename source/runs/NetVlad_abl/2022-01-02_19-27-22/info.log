2022-01-02 19:27:22   Arguments: Namespace(cache_refresh_rate=1000, datasets_folder='../data/', device='cuda', epochs_num=20, exp_name='./runs/NetVlad_abl', gem_power=None, infer_batch_size=16, lr=1e-05, margin=0.1, neg_samples_num=1000, negs_num_per_query=10, netvlad_clusters=64, num_workers=4, output_folder='runs/./runs/NetVlad_abl/2022-01-02_19-27-22', patience=3, queries_per_epoch=3000, recall_values=[1, 5, 10, 20], seed=0, train_batch_size=2, train_positives_dist_threshold=10, val_positive_dist_threshold=25)
2022-01-02 19:27:22   The outputs are being saved in runs/./runs/NetVlad_abl/2022-01-02_19-27-22
2022-01-02 19:27:22   Using 1 GPUs and 8 CPUs
2022-01-02 19:27:22   There are 96 queries without any positives within the training set. They won't be considered as they're useless for training.
2022-01-02 19:27:22   Train query set: < TripletsDataset, pitts30k - #database: 10000; #queries: 7320 >
2022-01-02 19:27:22   Val set: < BaseDataset, pitts30k - #database: 10000; #queries: 7608 >
2022-01-02 19:27:22   Test set: < BaseDataset, pitts30k - #database: 10000; #queries: 6816 >
2022-01-02 19:27:24   Output dimension of the model is 16384
2022-01-02 19:27:24   Start training epoch: 00
2022-01-02 20:09:02   Finished epoch 00 in 0:41:37, average epoch triplet loss = 0.0985
2022-01-02 20:15:41   Recalls on val set < BaseDataset, pitts30k - #database: 10000; #queries: 7608 >: R@1: 12.6, R@5: 28.8, R@10: 40.4, R@20: 54.2
2022-01-02 20:15:42   Improved: previous best R@5 = 0.0, current R@5 = 28.8
2022-01-02 20:15:42   Start training epoch: 01
2022-01-02 20:57:25   Finished epoch 01 in 0:41:43, average epoch triplet loss = 0.0991
2022-01-02 21:04:02   Recalls on val set < BaseDataset, pitts30k - #database: 10000; #queries: 7608 >: R@1: 14.5, R@5: 32.8, R@10: 43.8, R@20: 57.1
2022-01-02 21:04:03   Improved: previous best R@5 = 28.8, current R@5 = 32.8
2022-01-02 21:04:03   Start training epoch: 02
2022-01-02 21:45:48   Finished epoch 02 in 0:41:44, average epoch triplet loss = 0.0987
2022-01-02 21:52:27   Recalls on val set < BaseDataset, pitts30k - #database: 10000; #queries: 7608 >: R@1: 13.6, R@5: 30.7, R@10: 43.3, R@20: 58.4
2022-01-02 21:52:28   Not improved: 1 / 3: best R@5 = 32.8, current R@5 = 30.7
2022-01-02 21:52:28   Start training epoch: 03
2022-01-02 22:34:06   Finished epoch 03 in 0:41:38, average epoch triplet loss = 0.0984
2022-01-02 22:40:42   Recalls on val set < BaseDataset, pitts30k - #database: 10000; #queries: 7608 >: R@1: 14.3, R@5: 31.4, R@10: 42.5, R@20: 56.6
2022-01-02 22:40:42   Not improved: 2 / 3: best R@5 = 32.8, current R@5 = 31.4
2022-01-02 22:40:42   Start training epoch: 04
2022-01-02 23:22:13   Finished epoch 04 in 0:41:30, average epoch triplet loss = 0.0982
2022-01-02 23:28:54   Recalls on val set < BaseDataset, pitts30k - #database: 10000; #queries: 7608 >: R@1: 16.4, R@5: 34.8, R@10: 46.1, R@20: 59.3
2022-01-02 23:28:55   Improved: previous best R@5 = 32.8, current R@5 = 34.8
2022-01-02 23:28:55   Start training epoch: 05
2022-01-03 00:10:35   Finished epoch 05 in 0:41:39, average epoch triplet loss = 0.0983
2022-01-03 00:17:13   Recalls on val set < BaseDataset, pitts30k - #database: 10000; #queries: 7608 >: R@1: 15.2, R@5: 32.6, R@10: 43.8, R@20: 58.2
2022-01-03 00:17:14   Not improved: 1 / 3: best R@5 = 34.8, current R@5 = 32.6
2022-01-03 00:17:14   Start training epoch: 06
2022-01-03 00:58:49   Finished epoch 06 in 0:41:34, average epoch triplet loss = 0.0980
2022-01-03 01:05:28   Recalls on val set < BaseDataset, pitts30k - #database: 10000; #queries: 7608 >: R@1: 15.7, R@5: 34.0, R@10: 46.5, R@20: 60.0
2022-01-03 01:05:29   Not improved: 2 / 3: best R@5 = 34.8, current R@5 = 34.0
2022-01-03 01:05:29   Start training epoch: 07
2022-01-03 01:43:10   
Traceback (most recent call last):
  File "/media/paolo/Elements/my_work/DropGeo/source/train.py", line 104, in <module>
    batch_loss = loss_triplet.item()
RuntimeError: CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

